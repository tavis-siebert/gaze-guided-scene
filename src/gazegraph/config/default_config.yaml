# Base directories
base:
  repo_root: ${REPO_ROOT}
  scratch_dir: /tmp # Will be overwritten by other configs

# Data directories
directories:
  data_dir: ${base.scratch_dir}/data
  egtea: ${directories.data_dir}/egtea_gaze
  graphs: ${directories.data_dir}/graphs
  traces: ${directories.data_dir}/traces
  features: ${directories.data_dir}/features

  # Scratch directories
  scratch:
    root: ${base.scratch_dir}
    egtea: ${directories.scratch.root}/egtea_gaze
    ego_topo: ${directories.scratch.root}/ego-topo
    tmp: ${directories.scratch.root}/tmp

# Dataset paths and settings
dataset:
  # EGTEA Gaze+ dataset paths
  egtea:
    raw_videos: ${directories.scratch.egtea}/raw_videos
    cropped_videos: ${directories.scratch.egtea}/cropped_videos
    gaze_data: ${directories.egtea}/gaze_data/gaze_data
    action_annotations: ${directories.egtea}/action_annotation
    noun_idx_file: ${dataset.egtea.action_annotations}/noun_idx.txt
    verb_idx_file: ${dataset.egtea.action_annotations}/verb_idx.txt
  
  # Sampling settings for graph dataset
  sampling:
    # Sampling strategy for selecting graph checkpoints or frames:
    #   - 'all': Use all valid samples (checkpoints or frames), ignoring samples_per_video.
    #   - 'uniform': Evenly spaced samples across valid checkpoints or frames, up to samples_per_video.
    #   - 'random': Random samples from valid checkpoints or frames, up to samples_per_video.
    # If samples_per_video >= number of available samples, all are used (with or without duplicates).
    # If allow_duplicates is true, repeated samples are allowed for 'uniform' and 'random' if needed.
    # If oversampling is true, sampling is performed over all valid frames (using latest checkpoint for each frame),
    #   otherwise only checkpoint frames are used.
    strategy: all  # 'all', 'uniform', or 'random'
    samples_per_video: 0  # number of samples per video; 0 = use all
    allow_duplicates: false  # whether sampling can include the same sample multiple times
    random_seed: 42  # seed for reproducible sampling; null for no fixed seed
    oversampling: false  # whether to sample all frames (not just checkpoint frames)

  # Ego-topo data paths
  ego_topo:
    data_dir: ${directories.scratch.ego_topo}/data/gtea
    splits:
      train: ${dataset.ego_topo.data_dir}/split/train_S1.csv
      val: ${dataset.ego_topo.data_dir}/split/val_S1.csv
      train_test: ${dataset.ego_topo.data_dir}/train_test_splits.json
      train_video_lengths: ${dataset.ego_topo.data_dir}/split/train_S1_nframes.csv
      val_video_lengths: ${dataset.ego_topo.data_dir}/split/val_S1_nframes.csv
      
  # Embeddings settings
  embeddings:
    object_label_embedding_path: ${directories.data_dir}/datasets/noun_label_embeddings.pth
    action_label_embedding_path: ${directories.data_dir}/datasets/action_label_embeddings.pth
    max_visit_sample: 3  # Maximum number of visits to sample per node (0 = use all visits)

# Model settings and paths
models:
  clip:
    model_dir: ${directories.scratch.egtea}/clip_model
    model_id: "openai/clip-vit-base-patch16"
  yolo_world:
    model_path: ${directories.scratch.egtea}/yolo_world_model/yolov8x-worldv2.onnx
    image_size: 640
    conf_threshold: 0.15
    iou_threshold: 0.5

# Training
training:
  batch_size: 128
  num_epochs: 200
  optimizer: "AdamW"
  lr: 0.001
  num_classes: 106
  num_heads: 4
  hidden_dim: 512
  num_layers: 4
  res_connect: false
  node_drop_p: 0
  max_nodes_droppable: 4
  # Validation set sampling timestamps (as ratios of video length)
  val_timestamps: [0.25, 0.5, 0.75]

# External resources
external:
  urls:
    dropbox_cropped_clips: "https://www.dropbox.com/scl/fi/97r0kjz65wb6xf0mjpcd0/video_clips.tar?rlkey=flcqqd91lyxtm6nlsh4vjzvkq&e=1&dl=0"
    dropbox_video_links: "https://www.dropbox.com/scl/fi/o7mrc7okncgoz14a49e5q/video_links.txt?rlkey=rcz1ffw4eoibod8mmyj1nmyot&e=1&dl=0"
    ego_topo_repo: "https://github.com/facebookresearch/ego-topo.git"
    yolo_world_model_onnx: "https://github.com/Ziad-Algrafi/ODLabel/raw/main/assets/yolov8x-worldv2.onnx?download="
    yolo_world_model_ultralytics: "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x-worldv2.pt"

# Processing settings
processing:
  n_cores: 2
  dataloader_workers: 2

# Gaze preprocessing settings 
gaze:
  preprocess_gaze: True # Whether to proprecess the gaze data
  fixation_window_threshold: 4  # Minimum number of consecutive fixation frames to treat as a true fixation
  reclassify_saccades: False # Whether to convert spatially stable saccades into fixations
  spatial_window_size: 3 # Number of frames used to compute spatial stability
  spatial_distance_threshold: 0.05 # Max median distance allowed for spatial stability
  interpolate_unknown: True # Whether to interpolate UNKNOWN gaze points
  unknown_neighbor_distance: 0.2 # Max distance between neighbors to treat as fixation
  
# Graph and fixation detection settings
graph:
  fixated_object_detection:
    # Basic fixation parameters
    min_fixation_frame_threshold: 4 # Mininum number of frames an object must be fixated on during the fixation period
    min_fixation_frame_ratio: 0.5  # Minimum fraction of frames an object must be fixated on during the fixation period, relative to the total number of frames any object was fixated
    bbox_margin: 10 # Number of pixels to increase bounding boxes for detecting gaze intersection

    # Weights for different components of fixation score
    weights:
      # Weight for detection duration (higher gives more importance to objects fixated for longer periods)
      duration: 1.0
      
      # Weight for bounding box stability (higher gives more importance to objects with stable positions)
      bbox_stability: 1.0
      
      # Weight for gaze proximity (higher gives more importance to objects closer to gaze point)
      gaze_proximity: 1.0
      
      # Weight for detection confidence (higher gives more importance to confident detections)
      confidence: 1.0
    
    # Thresholds for component scores (objects with scores below thresholds will be filtered out)
    thresholds:
      # Bounding box stability threshold (0.0-1.0)
      # Higher values require more stable bounding boxes
      # 0.0 = No stability required, 1.0 = Perfect stability required (identical boxes)
      bbox_stability: 0
      
      # Gaze proximity threshold (0.0-1.0)
      # Higher values require gaze points to be closer to bbox centers
      # 0.0 = No proximity requirement, 1.0 = Very close proximity required
      # Value is normalized with 1.0/(1.0 + distance), so closer distances = higher values
      gaze_proximity: 0
      
      # Confidence threshold (0.0-1.0)
      # Higher values require more confident detections
      # This is the geometric mean of detection confidences across frames
      confidence: 0.3